{
  "TMA-Segmentation-Crop": "# Cellpose-SAM Tissue Core Segmentation\n\nAutomated tissue core detection and segmentation for tissue microarray (TMA) extraction\n\n\nIt  automatically segments and crops individual tissue cores from microscopy images. \n\n## Setup Instructions\n\n\n### 1. Create a Conda Env\n\n\n```bash\n# Create a new conda environment with Python 3.9\nconda create -n cellpose_env python=3.9\n\n# Activate the environment\nconda activate cellpose_env\n```\n\n### 2. Install Dependencies\n\nInstall the required Python packages using the `requirements.txt` file.\n\n```bash\npip install -r requirements.txt\n```\n\n\n## Basic Usage\n\n```bash\npython simple_main.py --input your_image.tif --output results\n\n\n!!! Note, you'll most likely have to adjust the downsampling factor. ~8 works well for the thumbnail stain images, and the default works bettetr for the ~ 30k pixel image.\n\n```\n\n## Example with Performance Assessment\n\n```bash\npython simple_main.py --input tissue_sample.tif --output results --expected-cores 20\n```\n\n## CLI Parameters\n\n| Parameter | Flag | Type | Default | Description | Recommended Value |\n|-----------|------|------|---------|-------------|------------------|\n| **Input Image** | `--input`, `-i` | string | Required | Path to input tissue image (supports TIFF, PNG, JPG) | `work/inputs/tissue_dapi_fullres.tif` |\n| **Output Directory** | `--output`, `-o` | string | `simple_output` | Directory for all output files and crops | `results` |\n| **Downsample Factor** | `--downsample` | int | 8 | Factor to reduce image size for processing (higher = faster, lower = more detail) | `64` |\n| **Expected Cores** | `--expected-cores` | int | 20 | Number of tissue cores expected in the image (for performance assessment) | `20` |\n| **Minimum Radius %** | `--min-radius-pct` | float | 1.0 | Minimum core radius as percentage of image width (filters small detections) | 1.0 |\n| **Maximum Radius %** | `--max-radius-pct` | float | 30.0 | Maximum core radius as percentage of image width (filters large detections) | 30.0 |\n| **Flow Threshold** | `--flow-threshold` | float | 0.4 | Cellpose flow threshold - higher values reduce false positives | `0.6` |\n| **Cell Probability Threshold** | `--cellprob-threshold` | float | 0.0 | Cellpose cell probability threshold - higher values reduce noise detections | `0.3` |\n| **Flat Field Correction** | `--flat-field` | flag | True | Enable flat field correction (illumination normalization).  | Default ON |\n\n\n## Processing Pipeline\n\nRaw Image \u2192 Format Conversion \u2192 Flat Field Correction \u2192 Downsampling \u2192 \nContrast Enhancement \u2192 Noise Reduction \u2192 Processed Image\n\n",
  "tinseltree": "# Tinseltree\n\n[![PyPI version](https://badge.fury.io/py/tinseltree.svg)](https://badge.fury.io/py/tinseltree)\n\n**Autonomous ML Experimentation System**\n\nTinseltree turns overnight failures into completed results by autonomously repairing the fragile glue code (paths, dependencies, configs) that causes ~70% of ML training crashes.\n\n## Quick Start\n\n### 1. Install\n\n```bash\npip install tinseltree\n```\n\n**For Development:**\n```bash\ngit clone https://github.com/yourusername/tinseltree.git\ncd tinseltree\npip install -e \".[dev]\"\n```\n\n### 2. Run Interactive Setup\n\n```bash\ntinseltree setup\n```\n\nThe setup wizard will guide you through configuring:\n- **Experiment Tracking** (wandb)\n- **Notifications** (Discord webhooks) \n- **Auto-fix** (OpenCode detection)\n- **SLURM** (default compute settings)\n\n### 3. Create Your First Experiment\n\n```bash\n# Create experiment directory\nmkdir -p experiments\n\n# Create a spec (or copy the example)\ncp experiments/example.yaml experiments/my_experiment.yaml\n\n# Edit to point to your training script\nnano experiments/my_experiment.yaml\n```\n\n### 4. Run It!\n\n```bash\ntinseltree run experiments/my_experiment.yaml\n```\n\n## How It Works\n\n1. **Write a spec** describing your experiment\n2. **Tinseltree instruments** your `train.py` with Prefect (for structured failures) + wandb (for tracking)\n3. **Submit to SLURM** with automatic monitoring\n4. **Auto-fix failures** using nested fix-it agents\n5. **Get notified** on Discord when done\n\n## Example Spec\n\n```yaml\nexperiment:\n  id: \"exp-001-mse-loss\"\n  goal: \"Switch from CrossEntropy to MSE loss\"\n  script: \"train.py\"\n\n  budget:\n    max_attempts: 3\n    max_cost: 25.00\n\n  training:\n    slurm_partition: \"gpu-short\"\n    time_limit: \"2:00:00\"\n    gpus: 1\n\n  success_criteria:\n    min_epochs: 10\n    max_val_mae: 0.45\n```\n\nSee `experiments/example.yaml` for a complete spec with all options.\n\n## Commands\n\n```bash\ntinseltree setup                  # Run interactive setup wizard\ntinseltree run <spec.yaml>        # Full workflow\ntinseltree instrument <spec.yaml> # Just instrument (for review)\ntinseltree status <experiment-id> # Check status\ntinseltree list                   # List all experiments\n```\n\n## Configuration\n\nThe setup wizard creates `~/.tinseltree/config.yaml` with your preferences. You can also:\n\n- **Manual config**: Copy `tinseltree/config_template.yaml` to `~/.tinseltree/config.yaml`\n- **Per-experiment**: Override defaults in experiment specs\n- **Environment variables**: Use `${VAR_NAME}` in configs\n\n### Manual Setup (Alternative to `tinseltree setup`)\n\nIf you prefer to configure manually:\n\n```bash\n# 1. Configure wandb\nwandb login\n\n# 2. Set Discord webhook (optional)\nexport DISCORD_WEBHOOK_URL=\"https://discord.com/api/webhooks/...\"\n\n# 3. Install OpenCode for auto-fix (optional)\nnpm i -g opencode-ai@latest\n# or: brew install anomalyco/tap/opencode\n\n# 4. Create config file\nmkdir -p ~/.tinseltree\ncp tinseltree/config_template.yaml ~/.tinseltree/config.yaml\n# Edit ~/.tinseltree/config.yaml as needed\n```\n\n## Requirements\n\n- Python 3.10+\n- SLURM cluster access\n- wandb account (free tier works)\n- **Optional**: OpenCode for auto-fix (`npm i -g opencode-ai@latest`)\n- **Optional**: Discord webhook for notifications\n\n## License\n\nMIT\n",
  "opencode-manager": "# OpenCode Manager\n\nMobile-first web interface for OpenCode AI agents. Manage, control, and code with OpenCode from any device - your phone, tablet, or desktop. Features Git integration, file management, and real-time chat in a responsive PWA. Deploy with Docker for instant setup. View diffs, edit files and much more.  \n\n## Features\n\n### Repository Management\n- **Multi-Repository Support** - Clone and manage multiple git repos/worktrees in local workspaces\n- **Private Repository Support** - GitHub PAT configuration for cloning private repos\n- **Worktree Support** - Create and manage Git worktrees for working on multiple branches\n\n### Git Integration\n- **Git Diff Viewer** - View file changes with unified diff, line numbers, and addition/deletion counts\n- **Git Status Panel** - See all uncommitted changes (modified, added, deleted, renamed, untracked)\n- **Branch Switching** - Switch between branches via dropdown\n- **Branch/Worktree Creation** - Create new branch workspaces from any repository\n- **Ahead/Behind Tracking** - Shows commits ahead/behind remote\n- **Push PRs to GitHub** - Create and push pull requests directly from your phone\n\n### File Browser\n- **Directory Navigation** - Browse files and folders with tree view\n- **File Search** - Search files within directories\n- **Syntax Highlighting** - Code preview with syntax highlighting\n- **File Operations** - Create files/folders, rename, delete\n- **Drag-and-Drop Upload** - Upload files by dragging into the browser\n- **Large File Support** - Virtualization for large files\n- **ZIP Download** - Download repos as ZIP excluding gitignored files\n\n### Chat & Session Features\n- **Slash Commands** - Built-in commands (`/help`, `/new`, `/models`, `/export`, `/compact`, etc.)\n- **Custom Commands** - Create custom slash commands with templates\n- **File Mentions** - Reference files with `@filename` autocomplete\n- **Plan/Build Mode Toggle** - Switch between read-only and file-change modes\n- **Mermaid Diagram Support** - Visual diagram rendering in chat messages\n- **Session Management** - Create, search, delete, and bulk delete sessions\n- **Real-time Streaming** - Live message streaming with SSE\n\n### AI Model & Provider Configuration\n- **Model Selection** - Browse and select from available AI models with filtering\n- **Provider Management** - Configure multiple AI providers with API keys or OAuth\n- **OAuth Authentication** - Secure OAuth login for supported providers (Anthropic, GitHub Copilot)\n- **Context Usage Indicator** - Visual progress bar showing token usage\n- **Agent Configuration** - Create custom agents with system prompts and tool permissions\n\n### MCP Server Management\n- **MCP Server Configuration** - Add local (command-based) or remote (HTTP) MCP servers\n- **Server Templates** - Pre-built templates for common MCP servers\n- **Enable/Disable Servers** - Toggle servers on/off with auto-restart\n\n### Settings & Customization\n- **Theme Selection** - Dark, Light, or System theme\n- **Keyboard Shortcuts** - Customizable keyboard shortcuts\n- **OpenCode Config Editor** - Raw JSON editor for advanced configuration\n\n### Mobile & PWA\n- **Mobile-First Design** - Responsive UI optimized for mobile use\n- **PWA Support** - Installable as Progressive Web App\n- **iOS Keyboard Support** - Proper keyboard handling on iOS\n- **Enter Key Send** - Press Enter to automatically close keyboard and send messages\n- **Swipe-to-Navigate** - Swipe right from left edge to navigate back\n\n### Text-to-Speech (TTS)\n- **Dual Provider Support** - Browser-native Web Speech API + external OpenAI-compatible endpoints\n- **Browser-Native TTS** - Built-in Web Speech API for instant playback without API keys\n- **AI Message Playback** - Listen to assistant responses with TTS\n- **OpenAI-Compatible** - Works with any OpenAI-compatible TTS endpoint\n- **Voice & Speed Discovery** - Automatic voice detection with caching (1hr TTL)\n- **Voice & Speed Controls** - Configurable voice selection and playback speed\n- **Audio Caching** - 24-hour cache with 200MB limit for performance\n- **Markdown Sanitization** - Filters unreadable symbols for smooth playback\n- **Floating Controls** - Persistent stop button for audio control\n- **Custom Endpoints** - Connect to local or self-hosted TTS services\n\n## Screenshots\n\n<table>\n<tr>\n<td><strong>Files (Mobile)</strong></td>\n<td><strong>Files (Desktop)</strong></td>\n</tr>\n<tr>\n<td><img src=\"https://github.com/user-attachments/assets/24243e5e-ab02-44ff-a719-263f61c3178b\" alt=\"files-mobile\" /></td>\n<td><img src=\"https://github.com/user-attachments/assets/0a37feb0-391c-48a1-8bda-44a046aad913\" alt=\"files-desktop\" /></td>\n</tr>\n<tr>\n<td><strong>Chat (Mobile)</strong></td>\n<td><strong>Chat (Desktop)</strong></td>\n</tr>\n<tr>\n<td><img src=\"https://github.com/user-attachments/assets/a48cc728-e540-4247-879a-c5f36c3fd6de\" alt=\"chat-mobile\" width=\"250\" /></td>\n<td><img src=\"https://github.com/user-attachments/assets/5fe34443-1d06-4847-a397-ef472aae0932\" alt=\"chat-desktop\" width=\"600\" /></td>\n</tr>\n<tr>\n<td><strong>Inline Diff View</strong></td>\n<td></td>\n</tr>\n<tr>\n<td><img src=\"https://github.com/user-attachments/assets/b94c0ca0-d960-4888-8a25-a31ed6d5068d\" alt=\"inline-diff-view\" width=\"250\" /></td>\n<td></td>\n</tr>\n</table>\n\n## Coming Soon\n\n-  **Authentication** - User authentication and session management\n\n## Installation\n\n### Option 1: Docker (Recommended for Production)\n\n```bash\n# Clone the repository\ngit clone https://github.com/chriswritescode-dev/opencode-manager.git\ncd opencode-manager\n\n# Start with Docker Compose (single container)\ndocker-compose up -d\n\n# Access the application at http://localhost:5001\n```\n\nThe Docker setup automatically:\n- Installs OpenCode if not present\n- Builds and serves frontend from backend\n- Sets up persistent volumes for workspace and database\n- Includes health checks and auto-restart\n\n**Docker Commands:**\n```bash\n# Start container\ndocker-compose up -d\n\n# Stop and remove container\ndocker-compose down\n\n# Rebuild image\ndocker-compose build\n\n# View logs\ndocker-compose logs -f\n\n# Restart container\ndocker-compose restart\n\n# Access container shell\ndocker exec -it opencode-manager sh\n```\n\n### Dev Server Ports\n\nThe Docker container exposes ports `5100-5103` for running dev servers inside your repositories. Configure your project's dev server to use one of these ports and access it directly from your browser.\n\n**Example usage:**\n```bash\n# Vite (vite.config.ts)\nserver: { port: 5100, host: '0.0.0.0' }\n\n# Next.js\nnext dev -p 5100 -H 0.0.0.0\n\n# Express/Node\napp.listen(5100, '0.0.0.0')\n```\n\nAccess your dev server at `http://localhost:5100` (or your Docker host IP).\n\nTo customize the exposed ports, edit `docker-compose.yml`:\n```yaml\nports:\n  - \"5003:5003\"      # OpenCode Manager\n  - \"5100:5100\"      # Dev server 1\n  - \"5101:5101\"      # Dev server 2\n  - \"5102:5102\"      # Dev server 3\n  - \"5103:5103\"      # Dev server 4\n```\n\n### Global Agent Instructions (AGENTS.md)\n\nOpenCode Manager creates a default `AGENTS.md` file in the workspace config directory (`/workspace/.config/opencode/AGENTS.md`). This file provides global instructions to AI agents working within the container.\n\n**Default instructions include:**\n- Reserved ports (5003 for OpenCode Manager, 5551 for OpenCode server)\n- Available dev server ports (5100-5103)\n- Guidelines for binding to `0.0.0.0` for Docker accessibility\n\n**Editing AGENTS.md:**\n- Via UI: Settings > OpenCode > Global Agent Instructions\n- Via file: Edit `/workspace/.config/opencode/AGENTS.md` directly\n\nThis file is merged with any repository-specific `AGENTS.md` files, with repository instructions taking precedence for their respective codebases.\n\n### Option 2: Local Development (Contributors)\n\nFor contributors who want to develop locally instead of using Docker.\n\n**Prerequisites:**\n- [pnpm](https://pnpm.io/installation) - Package manager (required for workspaces)\n- [Bun](https://bun.sh) - Backend runtime\n- [OpenCode TUI](https://opencode.ai) - `npm install -g @opencode/tui`\n\n```bash\n# Clone the repository\ngit clone https://github.com/chriswritescode-dev/opencode-manager.git\ncd opencode-manager\n\n# Install dependencies (uses pnpm workspaces)\npnpm install\n\n# Copy environment configuration\ncp .env.example .env\n\n# Start development servers (backend + frontend)\npnpm dev\n```\n\n## OAuth Provider Setup\n\nOpenCode WebUI supports OAuth authentication for select providers, offering a more secure and convenient alternative to API keys.\n\n### Supported OAuth Providers\n\n- **Anthropic (Claude)** - OAuth login with Claude Pro/Max accounts\n- **GitHub Copilot** - OAuth device flow authentication\n\n### Setting Up OAuth\n\n1. **Navigate to Settings \u2192 Provider Credentials**\n2. **Select a provider** that shows the \"OAuth\" badge\n3. **Click \"Add OAuth\"** to start the authorization flow\n4. **Choose authentication method:**\n   - **\"Open Authorization Page\"** - Opens browser for sign-in\n   - **\"Use Authorization Code\"** - Provides code for manual entry\n5. **Complete authorization** in the browser or enter the provided code\n6. **Connection status** will show as \"Configured\" when successful\n\n\n\n",
  "fingertest": "# Autonomous Authorship Verification System\n\nA robust, end-to-end authorship verification system that learns to map text styles to a vector space using metric learning with active hard-negative mining.\n\n## System Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     PHASE 1: Data Pipeline                      \u2502\n\u2502  Raw Discord JSON \u2192 Cleaning \u2192 Session Aggregation \u2192 Splits    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 PHASE 2: Baseline Training                      \u2502\n\u2502        RoBERTa + Mean Pooling + MNRL (In-batch Negatives)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           PHASE 3: Autonomous Hard-Negative Loop                \u2502\n\u2502  Train \u2192 FAISS Mining \u2192 Find Hard Negatives \u2192 Retrain (3x)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              PHASE 4: Forensic Evaluation                       \u2502\n\u2502         EER, ROC-AUC, UMAP Visualization (Zero-Shot)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Hardware Requirements\n\n- **GPU:** NVIDIA RTX 3090 (24GB VRAM) with CUDA 12\n- **RAM:** 32GB+ recommended\n- **Disk:** 20GB+ free space\n- **Python:** 3.11+ (required for faiss-gpu-cu12)\n\n## Quick Start\n\n### 1. Prepare Data\n\nPlace your Discord JSON dumps in `data/raw/`:\n```bash\ndata/raw/\n\u251c\u2500\u2500 server1/\n\u2502   \u251c\u2500\u2500 channel1.json\n\u2502   \u2514\u2500\u2500 channel2.json\n\u251c\u2500\u2500 server2/\n\u2502   \u2514\u2500\u2500 general.json\n...\n```\n\n**Expected JSON format:**\n```json\n[\n  {\n    \"author\": {\n      \"id\": \"123456789\",\n      \"username\": \"user123\",\n      \"bot\": false,\n      \"discriminator\": \"1234\"\n    },\n    \"content\": \"Message text here\",\n    \"timestamp\": \"2024-01-01T12:00:00.000Z\",\n    \"channel_id\": \"987654321\"\n  },\n  ...\n]\n```\n\n### 2. Run Full Pipeline\n\n```bash\n./run_full_pipeline.sh\n```\n\nThis will:\n1. Install dependencies\n2. Preprocess data\n3. Train baseline model\n4. Run autonomous mining loop (3 iterations)\n5. Evaluate on zero-shot test set\n\n### 3. Check Results\n\nResults will be in:\n- `outputs/final_evaluation/metrics.json` - Key metrics (EER, ROC-AUC)\n- `outputs/final_evaluation/*.png` - Visualizations\n- `models/loop/final_model/` - Trained model\n\n## Manual Execution\n\n### Phase 1: Data Preprocessing\n\n```bash\npython preprocess.py \\\n    --raw-dir data/raw \\\n    --output-dir data/processed \\\n    --min-blocks 5\n```\n\n**What it does:**\n- Streams 10GB+ JSON files without OOM\n- Removes bots and system messages\n- Normalizes URLs/mentions, keeps emojis\n- Aggregates messages into context blocks (20-512 tokens)\n- Creates stratified splits:\n  - Train: 90% of data from users with 5+ blocks\n  - Val: 10% of data from train users\n  - Test: 1000 held-out authors (zero-shot)\n\n### Phase 2: Baseline Training\n\n```bash\npython train_baseline.py \\\n    --train-data data/processed/train.parquet \\\n    --val-data data/processed/val.parquet \\\n    --output-dir models/baseline \\\n    --batch-size 64 \\\n    --epochs 1 \\\n    --lr 2e-5 \\\n    --fp16\n```\n\n**Architecture:**\n- Base: `roberta-base` (125M params)\n- Pooling: Mean pooling\n- Loss: MultipleNegativesRankingLoss (in-batch negatives)\n- Output: 768-dim normalized embeddings\n\n### Phase 3: Hard-Negative Mining\n\n**Step A: Mine hard negatives**\n```bash\npython miner.py \\\n    --model models/baseline \\\n    --data data/processed/train.parquet \\\n    --output data/processed/hard_negatives.parquet \\\n    --sample-size 50000 \\\n    --k 10 \\\n    --prioritize-same-channel\n```\n\n**What it does:**\n- Encodes 50k random training samples\n- Builds FAISS index (GPU-accelerated)\n- For each sample, finds top-10 nearest neighbors\n- Identifies different-author neighbors as \"hard negatives\"\n- Prioritizes same-channel negatives (kills topic bias)\n\n**Step B: Fine-tune on triplets**\n```bash\npython train_triplet.py \\\n    --model models/baseline \\\n    --triplets data/processed/hard_negatives.parquet \\\n    --output models/triplet_refined \\\n    --batch-size 32 \\\n    --epochs 1 \\\n    --lr 1e-5 \\\n    --margin 0.5 \\\n    --fp16\n```\n\n**Loss:** TripletMarginLoss with margin=0.5\n\n### Autonomous Loop\n\n```bash\npython run_loop.py \\\n    --base-model models/baseline \\\n    --data-dir data/processed \\\n    --output models/loop \\\n    --iterations 3 \\\n    --sample-size 50000 \\\n    --mining-k 10 \\\n    --batch-size 32 \\\n    --lr 1e-5 \\\n    --margin 0.5 \\\n    --fp16\n```\n\nThis automatically runs:\n```\nIteration 1: Mine \u2192 Train \u2192 Evaluate\nIteration 2: Mine \u2192 Train \u2192 Evaluate\nIteration 3: Mine \u2192 Train \u2192 Evaluate\n```\n\n### Phase 4: Evaluation\n\n```bash\npython evaluate.py \\\n    --model models/loop/final_model \\\n    --test-data data/processed/test.parquet \\\n    --output outputs/final_evaluation \\\n    --num-positive 2000 \\\n    --num-negative 2000\n```\n\n**Metrics computed:**\n- **EER (Equal Error Rate):** Target < 0.05\n- **ROC-AUC:** Area under ROC curve\n- **Accuracy at EER threshold**\n\n**Visualizations:**\n- `roc_curve.png` - ROC curve\n- `far_frr_curves.png` - FAR/FRR with EER point\n- `score_distribution.png` - Similarity distributions\n- `umap_visualization.png` - 2D embedding space\n\n## Key Design Decisions\n\n### 1. Streaming Data Pipeline\n- Uses generators and HuggingFace datasets to handle 10GB+ without RAM overflow\n- Processes in chunks of 10k messages\n\n### 2. Session Aggregation\n- Groups consecutive messages from (user, channel) if \u0394t < 5 minutes\n- Creates context-rich blocks (20-512 tokens)\n- Better than single messages for style learning\n\n### 3. Zero-Shot Test Set\n- Bottom 1000 authors completely held out\n- Model never sees these users during training\n- Tests true authorship verification capability\n\n### 4. Hard-Negative Mining\n- Uses FAISS for fast similarity search (GPU-accelerated)\n- Finds \"confusing\" examples (different authors, similar style)\n- Prioritizes same-channel negatives to reduce topic bias\n\n### 5. Autonomous Loop\n- Iteratively improves by finding its own mistakes\n- Each iteration:\n  1. Identifies hard negatives (model's errors)\n  2. Retrains on these hard cases\n  3. Evaluates on zero-shot test\n\n### 6. RoBERTa over BERT\n- Better at byte-level noise handling\n- More robust to Discord's informal text style\n\n## File Structure\n\n```\n.\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                    # Place Discord JSON here\n\u2502   \u2514\u2500\u2500 processed/              # Parquet files (train/val/test)\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 baseline/               # Initial MNRL model\n\u2502   \u2514\u2500\u2500 loop/                   # Autonomous loop outputs\n\u2502       \u251c\u2500\u2500 iteration_1/\n\u2502       \u251c\u2500\u2500 iteration_2/\n\u2502       \u251c\u2500\u2500 iteration_3/\n\u2502       \u251c\u2500\u2500 final_model/        # Best model\n\u2502       \u2514\u2500\u2500 results.json        # Metrics per iteration\n\u251c\u2500\u2500 outputs/\n\u2502   \u251c\u2500\u2500 baseline_evaluation/    # Baseline metrics\n\u2502   \u2514\u2500\u2500 final_evaluation/       # Final metrics + visualizations\n\u251c\u2500\u2500 preprocess.py               # Phase 1: Data pipeline\n\u251c\u2500\u2500 train_baseline.py           # Phase 2: Baseline trainer\n\u251c\u2500\u2500 miner.py                    # Phase 3A: Hard-negative mining\n\u251c\u2500\u2500 train_triplet.py            # Phase 3B: Triplet training\n\u251c\u2500\u2500 run_loop.py                 # Phase 3: Autonomous loop\n\u251c\u2500\u2500 evaluate.py                 # Phase 4: Evaluation\n\u251c\u2500\u2500 run_full_pipeline.sh        # Master script\n\u2514\u2500\u2500 requirements.txt            # Python dependencies\n```\n\n## Performance Tuning\n\n### Batch Size Optimization\nAuto-calculated based on VRAM:\n- Baseline training: 64-128 (MNRL)\n- Triplet training: 32 (TripletLoss has 3x overhead)\n\n### Memory Management\n- Use `fp16=True` for 2x memory savings\n- FAISS index on GPU for fast mining\n- Streaming encoders for large corpora\n\n### Hyperparameters\nTested defaults:\n- Learning rate: 2e-5 (baseline), 1e-5 (refinement)\n- Triplet margin: 0.5\n- Mining k: 10 neighbors\n- Sample size: 50k per iteration\n\n## Expected Results\n\n| Metric | Target | Typical |\n|--------|--------|---------|\n| EER | < 0.05 | 0.03-0.06 |\n| ROC-AUC | > 0.95 | 0.94-0.98 |\n| Same-Channel Negatives | > 30% | 35-45% |\n\n## Troubleshooting\n\n### Out of Memory (OOM)\n- Reduce `--batch-size`\n- Reduce `--sample-size` in mining\n- Ensure `--fp16` is enabled\n\n### No Data Found\n- Check `data/raw/` has .json files\n- Verify JSON format matches expected structure\n\n### Poor EER (> 0.10)\n- Increase `--iterations` in loop\n- Increase `--min-blocks` (more data per author)\n- Check data quality (enough authors?)\n\n### FAISS GPU Error\n- Install: `uv pip install faiss-gpu-cu12` (CUDA 12)\n- Verify: `uv run python -c \"import faiss; print(faiss.get_num_gpus())\"`\n- Fallback to CPU: `--no-gpu` flag in miner.py\n\n## Citation\n\nIf you use this system, please cite:\n```\nAutonomous Authorship Verification System\nHard-Negative Mining Loop with RoBERTa + FAISS\n```\n\n## License\n\nMIT\n"
}