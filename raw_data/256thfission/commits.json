[
  "Add sortable columns, stars display, toxicity percentage, and badges tray to scoreboard table",
  "update",
  "Inital push",
  "version bump",
  "Basic version 0.0.1",
  "Git ignore",
  "inital spam",
  "docfix",
  "Merge pull request #4 from 256thFission/fix-catalog-only-scraper\n\nFix catalog only scraper",
  "I am very confused",
  "it works good enough.",
  "bigtest",
  "Simplify scraper to read all departments from area_codes.csv\n\nMajor simplification:\n- Removed hardcoded DEPARTMENTS dictionary usage\n- Now reads all departments from utils/area_codes.csv (298 departments)\n- Searches by course code pattern (e.g., 'COMPSCI-') instead of area_id\n- This is more reliable as area_ids are inconsistent across departments\n- Much simpler workflow: load CSV, search each dept by pattern, save results\n- Better progress output showing X/Y departments processed\n- Always clears evaluations list between departments (fixes stale data issue)",
  "Fix: Always clear evaluations list between departments\n\n- Moved scraper.evaluations = [] outside the 'if results:' block\n- Ensures list is cleared even when a department has 0 results\n- Prevents stale data from carrying over to next department search",
  "Fix evaluation scraper to handle cross-listed courses properly\n\nProblem:\n- Cross-listed courses only appeared under one department area in search results\n- Example: CULANTH-190S/AAAS-190S/AMES-190S/ICS-190S only showed under CULANTHRO\n- Previous approach of searching by course code missed many evaluations\n\nSolution:\n1. Added extract_department_codes_from_title() function to parse all cross-listed\n   department codes from course titles using regex pattern matching\n\n2. Updated _parse_search_result() to extract and store department_codes list\n   for each evaluation\n\n3. Modified download_report() to save each evaluation to ALL relevant department\n   folders based on extracted codes (e.g., saves to both CULANTH/ and AAAS/ folders)\n\n4. Simplified search approach in example script:\n   - Search by department area_id only (course parameter = \"\")\n   - Automatically handles cross-listing through title parsing\n   - Reports saved to multiple folders transparently\n\n5. Updated CSV export to include department_codes field showing all cross-listed departments\n\nAll tests pass for extracting department codes from various title formats.",
  "Restore parse_course_evaluations.py from 2929c1f (undo bad merge in c4f9f91)",
  "Merge pull request #3 from 256thFission/eval-site-big-fix\n\nEval site big fix",
  "test merge",
  "Output CSV files per department instead of combined\n\nChanges:\n- Each department now gets its own set of CSV files\n- Files are written to: data/course_evaluations/DEPARTMENT/\n- Structure:\n  - data/course_evaluations/COMPSCI/evaluations_responses.csv\n  - data/course_evaluations/COMPSCI/evaluations_questions.csv\n  - data/course_evaluations/COMPSCI/evaluations_free_text.csv\n  - data/course_evaluations/MATH/evaluations_responses.csv\n  - etc.\n\nBenefits:\n- Easier to manage department-specific data\n- Keeps evaluation data with source HTML files\n- Can process departments independently\n- Better organization for large datasets\n\nOutput now shows:\n  Processing COMPSCI (45 courses)\n  [1/45] COMPSCI-512-01_Maggs_Bruce_Fall_2024.html\n  ...\n  Writing CSV files to data/course_evaluations/COMPSCI/...\n  \u2713 evaluations_responses.csv (1530 rows)",
  "Fixed scraping",
  "Improve parser UX and multi-department support\n\nUsability improvements:\n- Default to 'data' directory if no argument provided\n- Just run: python parse_course_evaluations.py\n- Show clear expected directory structure in error messages\n- Display department-by-department processing with headers\n- Group file discovery by department with counts\n\nBetter output:\n- Show which department is being processed\n- Concise per-file output: course, instructor, Qs, comments\n- Department summary before parsing starts\n- Better error messages with example structure\n\nExample output:\n  Found 80 HTML files across 5 departments:\n    \u2022 COMPSCI: 45 courses\n    \u2022 MATH: 20 courses\n    \u2022 HISTORY: 15 courses\n\n  Processing COMPSCI\n  [1/80] COMPSCI-512-01_Maggs_Bruce_Fall_2024.html\n    \u2713 COMPSCI-512-01 - Bruce Maggs (9 Qs, 36 comments)",
  "Fix parser to handle multiple courses and auto-discover files\n\nMajor improvements:\n- Auto-discover HTML files in directory structure (data/**/reports/*.html)\n- Take directory path as input instead of individual files\n- Fix CSV overwriting - now collects all data first, writes once\n- Filter out junk text (footer links, navigation) from free text responses\n- Add progress tracking and error reporting\n- Support processing all 80+ courses automatically\n\nUsage: python parse_course_evaluations.py data/\n\nFixes:\n- Lines 15-40: Added junk text filtering\n- Lines 138, 145: Apply filters to free text extraction\n- Lines 197-281: New methods return rows instead of writing directly\n- Lines 284-298: Auto-discover files in directory structure\n- Lines 336-428: Collect all data, then write CSVs once at the end",
  "Add course evaluation HTML parser script\n\n- Parses Duke Course Evaluation HTML files into CSV format\n- Extracts course metadata, questions, response distributions, and statistics\n- Generates three CSV outputs:\n  - evaluations_responses.csv: detailed response-level data\n  - evaluations_questions.csv: question-level summary\n  - evaluations_free_text.csv: student free-text responses\n- Supports parsing multiple HTML files in a single run\n- Handles both quantitative ratings and qualitative text responses",
  "Add samples/remove har",
  "Merge pull request #2 from 256thFission/course-eval-pdf-scraper-test\n\nCourse eval pdf scraper test",
  "Fully functional pagination ckpt",
  "pre-overserbavility",
  "eval debug",
  "Merge PR for course evals",
  "Merge branch 'main' of https://github.com/256thFission/duke-catalog-scraper\naaaaa",
  "Complete course evaluation scraper with report downloading\n\nUpdates:\n- Set REPORT_URL to correct endpoint (StudentReport.aspx)\n- Implement correct URL construction with comma-separated IDs\n- Update download_report() to use proper URL format\n- Remove TODO warnings and enable full functionality\n- Update README to reflect completed implementation\n- Add comprehensive test suite for URL construction\n\nThe scraper now fully supports:\n- Searching evaluations by department/course/instructor/term\n- Extracting metadata with data-id values\n- Downloading HTML report files\n- Exporting to JSON/CSV\n\nAll tests pass. Ready for production use.",
  "Detailed scraper",
  "Add Duke Course Evaluation scraper\n\nImplements comprehensive scraper for Duke course evaluations:\n\nFeatures:\n- Cookie-based Shibboleth authentication\n- Department-wide evaluation search\n- Session keep-alive management\n- Metadata extraction and export (JSON/CSV)\n- HTML report downloading (pending URL pattern)\n\nComponents:\n- course_eval_scraper.py: Main scraper class\n- examples/scrape_course_evals.py: Example usage script\n- utils/extract_departments.py: Department ID extractor\n- README_COURSE_EVAL.md: Comprehensive documentation\n- departments.json: Complete department mappings (160+ depts)\n\nTODO: User needs to provide View Report URL pattern to enable\nreport downloading functionality.",
  "auth modifications",
  "add watermark har",
  "Initial commit",
  "Update README.md",
  "inital bs version",
  "Deleted env",
  "Update README.md",
  "Lin's Duke List Code",
  "updated pyipi config",
  "Merge branch 'main' of github.com:256thFission/ThinkMark\nAccidental readme merge conflict.",
  "Added module level tests, fixed bug in url parsing.",
  "Update README.md",
  "Update README.md",
  "Initial commit: ThinkMark v0.2.0 release",
  "Initial commit",
  "Manual only savestates",
  "Claude clean code yep",
  "Load n unload",
  "Smooth Mooves",
  "Working Base, no lobby.",
  "first commit started refactor oops",
  "first commit",
  "Initial commit from Create Next App",
  "innit",
  "Initial commit",
  "progres done",
  "LoadedMessages fail",
  "Right b4 jotai",
  "Load model",
  "Cardstack works",
  "time to rework",
  "New ML pass",
  "New ML pass",
  "Project innit",
  "Project innit",
  "nnit",
  "fine",
  "relog",
  "Fixed css",
  "defacto",
  "who looked",
  "boyfried",
  "you had a",
  "somebody told me",
  "Prod 5.5",
  "Prod 5.3",
  "Prod 5.2",
  "Prod 5.1",
  "Prod 5.0",
  "Prod 4.0",
  "Prod 3.3",
  "Prod v3.1",
  "Prod v2.1",
  "Prod v2.0",
  "Prod v1.2",
  "Prod v1.1",
  "Prod v1",
  "Vercel version 10",
  "Vercel version 9",
  "Vercel version 8",
  "Vercel version 9",
  "Vercel version 5",
  "Vercel version 4",
  "Vercel version 3",
  "baba booey",
  "Vercel version 2",
  "Vercel version 2",
  "Vercel version 2",
  "Vercel version",
  "Auth MVP",
  "The form wokrs",
  "rgwer",
  "test",
  "yep\n\nInital commit, accidentally remoevd style",
  "initial commit",
  "Initial commit from Create Next App",
  "yes",
  "Lab4 probably done",
  "This is a git test.",
  "RE\n\nI did stuff",
  "Templete removal",
  "Templete removal",
  "Templete removal",
  "Initial commit",
  "Initial commit"
]